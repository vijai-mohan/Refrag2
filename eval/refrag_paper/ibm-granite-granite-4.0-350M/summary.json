{
  "model": "ibm-granite/granite-4.0-350M",
  "run_name": "ibm-granite-granite-4.0-350M",
  "lm_eval": {
    "ok": true,
    "tasks": {
      "mathqa": {
        "acc,none": 0.1,
        "acc_stderr,none": 0.09999999999999999,
        "acc_norm,none": 0.2,
        "acc_norm_stderr,none": 0.13333333333333333
      },
      "hellaswag": {
        "acc,none": 0.3,
        "acc_stderr,none": 0.15275252316519464,
        "acc_norm,none": 0.3,
        "acc_norm_stderr,none": 0.15275252316519464
      },
      "piqa": {
        "acc,none": 0.7,
        "acc_stderr,none": 0.15275252316519466,
        "acc_norm,none": 0.8,
        "acc_norm_stderr,none": 0.13333333333333333
      },
      "winogrande": {
        "acc,none": 0.8,
        "acc_stderr,none": 0.13333333333333333
      },
      "mmlu": {
        "acc,none": 0.3456140350877193,
        "acc_stderr,none": 0.019994527482197377
      },
      "mmlu_humanities": {
        "acc,none": 0.34615384615384615,
        "acc_stderr,none": 0.04174056563102489
      },
      "mmlu_formal_logic": {
        "acc,none": 0.1,
        "acc_stderr,none": 0.09999999999999999
      },
      "mmlu_high_school_european_history": {
        "acc,none": 0.5,
        "acc_stderr,none": 0.16666666666666666
      },
      "mmlu_high_school_us_history": {
        "acc,none": 0.5,
        "acc_stderr,none": 0.16666666666666666
      },
      "mmlu_high_school_world_history": {
        "acc,none": 0.5,
        "acc_stderr,none": 0.16666666666666666
      },
      "mmlu_international_law": {
        "acc,none": 0.5,
        "acc_stderr,none": 0.16666666666666666
      },
      "mmlu_jurisprudence": {
        "acc,none": 0.2,
        "acc_stderr,none": 0.13333333333333333
      },
      "mmlu_logical_fallacies": {
        "acc,none": 0.1,
        "acc_stderr,none": 0.09999999999999999
      },
      "mmlu_moral_disputes": {
        "acc,none": 0.4,
        "acc_stderr,none": 0.16329931618554522
      },
      "mmlu_moral_scenarios": {
        "acc,none": 0.2,
        "acc_stderr,none": 0.13333333333333333
      },
      "mmlu_philosophy": {
        "acc,none": 0.3,
        "acc_stderr,none": 0.15275252316519464
      },
      "mmlu_prehistory": {
        "acc,none": 0.5,
        "acc_stderr,none": 0.16666666666666666
      },
      "mmlu_professional_law": {
        "acc,none": 0.3,
        "acc_stderr,none": 0.15275252316519464
      },
      "mmlu_world_religions": {
        "acc,none": 0.4,
        "acc_stderr,none": 0.16329931618554522
      },
      "mmlu_other": {
        "acc,none": 0.3384615384615385,
        "acc_stderr,none": 0.04134491152973615
      },
      "mmlu_business_ethics": {
        "acc,none": 0.2,
        "acc_stderr,none": 0.13333333333333333
      },
      "mmlu_clinical_knowledge": {
        "acc,none": 0.4,
        "acc_stderr,none": 0.16329931618554522
      },
      "mmlu_college_medicine": {
        "acc,none": 0.3,
        "acc_stderr,none": 0.15275252316519464
      },
      "mmlu_global_facts": {
        "acc,none": 0.6,
        "acc_stderr,none": 0.16329931618554522
      },
      "mmlu_human_aging": {
        "acc,none": 0.5,
        "acc_stderr,none": 0.16666666666666666
      },
      "mmlu_management": {
        "acc,none": 0.5,
        "acc_stderr,none": 0.16666666666666666
      },
      "mmlu_marketing": {
        "acc,none": 0.3,
        "acc_stderr,none": 0.15275252316519464
      },
      "mmlu_medical_genetics": {
        "acc,none": 0.3,
        "acc_stderr,none": 0.15275252316519464
      },
      "mmlu_miscellaneous": {
        "acc,none": 0.2,
        "acc_stderr,none": 0.13333333333333333
      },
      "mmlu_nutrition": {
        "acc,none": 0.4,
        "acc_stderr,none": 0.16329931618554522
      },
      "mmlu_professional_accounting": {
        "acc,none": 0.1,
        "acc_stderr,none": 0.09999999999999999
      },
      "mmlu_professional_medicine": {
        "acc,none": 0.5,
        "acc_stderr,none": 0.16666666666666666
      },
      "mmlu_virology": {
        "acc,none": 0.1,
        "acc_stderr,none": 0.09999999999999999
      },
      "mmlu_social_sciences": {
        "acc,none": 0.35,
        "acc_stderr,none": 0.04356774205932811
      },
      "mmlu_econometrics": {
        "acc,none": 0.5,
        "acc_stderr,none": 0.16666666666666666
      },
      "mmlu_high_school_geography": {
        "acc,none": 0.2,
        "acc_stderr,none": 0.13333333333333333
      },
      "mmlu_high_school_government_and_politics": {
        "acc,none": 0.5,
        "acc_stderr,none": 0.16666666666666666
      },
      "mmlu_high_school_macroeconomics": {
        "acc,none": 0.4,
        "acc_stderr,none": 0.16329931618554522
      },
      "mmlu_high_school_microeconomics": {
        "acc,none": 0.0,
        "acc_stderr,none": 0.0
      },
      "mmlu_high_school_psychology": {
        "acc,none": 0.3,
        "acc_stderr,none": 0.15275252316519464
      },
      "mmlu_human_sexuality": {
        "acc,none": 0.6,
        "acc_stderr,none": 0.16329931618554522
      },
      "mmlu_professional_psychology": {
        "acc,none": 0.4,
        "acc_stderr,none": 0.16329931618554522
      },
      "mmlu_public_relations": {
        "acc,none": 0.3,
        "acc_stderr,none": 0.15275252316519464
      },
      "mmlu_security_studies": {
        "acc,none": 0.3,
        "acc_stderr,none": 0.15275252316519464
      },
      "mmlu_sociology": {
        "acc,none": 0.4,
        "acc_stderr,none": 0.16329931618554522
      },
      "mmlu_us_foreign_policy": {
        "acc,none": 0.3,
        "acc_stderr,none": 0.15275252316519464
      },
      "mmlu_stem": {
        "acc,none": 0.3473684210526316,
        "acc_stderr,none": 0.03499989007589474
      },
      "mmlu_abstract_algebra": {
        "acc,none": 0.5,
        "acc_stderr,none": 0.16666666666666666
      },
      "mmlu_anatomy": {
        "acc,none": 0.2,
        "acc_stderr,none": 0.13333333333333333
      },
      "mmlu_astronomy": {
        "acc,none": 0.3,
        "acc_stderr,none": 0.15275252316519464
      },
      "mmlu_college_biology": {
        "acc,none": 0.3,
        "acc_stderr,none": 0.15275252316519464
      },
      "mmlu_college_chemistry": {
        "acc,none": 0.4,
        "acc_stderr,none": 0.16329931618554522
      },
      "mmlu_college_computer_science": {
        "acc,none": 0.5,
        "acc_stderr,none": 0.16666666666666666
      },
      "mmlu_college_mathematics": {
        "acc,none": 0.2,
        "acc_stderr,none": 0.13333333333333333
      },
      "mmlu_college_physics": {
        "acc,none": 0.3,
        "acc_stderr,none": 0.15275252316519464
      },
      "mmlu_computer_security": {
        "acc,none": 0.3,
        "acc_stderr,none": 0.15275252316519464
      },
      "mmlu_conceptual_physics": {
        "acc,none": 0.2,
        "acc_stderr,none": 0.13333333333333333
      },
      "mmlu_electrical_engineering": {
        "acc,none": 0.6,
        "acc_stderr,none": 0.16329931618554522
      },
      "mmlu_elementary_mathematics": {
        "acc,none": 0.2,
        "acc_stderr,none": 0.13333333333333333
      },
      "mmlu_high_school_biology": {
        "acc,none": 0.6,
        "acc_stderr,none": 0.16329931618554522
      },
      "mmlu_high_school_chemistry": {
        "acc,none": 0.5,
        "acc_stderr,none": 0.16666666666666666
      },
      "mmlu_high_school_computer_science": {
        "acc,none": 0.3,
        "acc_stderr,none": 0.15275252316519464
      },
      "mmlu_high_school_mathematics": {
        "acc,none": 0.4,
        "acc_stderr,none": 0.16329931618554522
      },
      "mmlu_high_school_physics": {
        "acc,none": 0.2,
        "acc_stderr,none": 0.13333333333333333
      },
      "mmlu_high_school_statistics": {
        "acc,none": 0.3,
        "acc_stderr,none": 0.15275252316519464
      },
      "mmlu_machine_learning": {
        "acc,none": 0.3,
        "acc_stderr,none": 0.15275252316519464
      }
    },
    "versions": {
      "mathqa": 1.0,
      "hellaswag": 1.0,
      "piqa": 1.0,
      "winogrande": 1.0,
      "mmlu": 2,
      "mmlu_abstract_algebra": 1.0,
      "mmlu_anatomy": 1.0,
      "mmlu_astronomy": 1.0,
      "mmlu_business_ethics": 1.0,
      "mmlu_clinical_knowledge": 1.0,
      "mmlu_college_biology": 1.0,
      "mmlu_college_chemistry": 1.0,
      "mmlu_college_computer_science": 1.0,
      "mmlu_college_mathematics": 1.0,
      "mmlu_college_medicine": 1.0,
      "mmlu_college_physics": 1.0,
      "mmlu_computer_security": 1.0,
      "mmlu_conceptual_physics": 1.0,
      "mmlu_econometrics": 1.0,
      "mmlu_electrical_engineering": 1.0,
      "mmlu_elementary_mathematics": 1.0,
      "mmlu_formal_logic": 1.0,
      "mmlu_global_facts": 1.0,
      "mmlu_high_school_biology": 1.0,
      "mmlu_high_school_chemistry": 1.0,
      "mmlu_high_school_computer_science": 1.0,
      "mmlu_high_school_european_history": 1.0,
      "mmlu_high_school_geography": 1.0,
      "mmlu_high_school_government_and_politics": 1.0,
      "mmlu_high_school_macroeconomics": 1.0,
      "mmlu_high_school_mathematics": 1.0,
      "mmlu_high_school_microeconomics": 1.0,
      "mmlu_high_school_physics": 1.0,
      "mmlu_high_school_psychology": 1.0,
      "mmlu_high_school_statistics": 1.0,
      "mmlu_high_school_us_history": 1.0,
      "mmlu_high_school_world_history": 1.0,
      "mmlu_human_aging": 1.0,
      "mmlu_human_sexuality": 1.0,
      "mmlu_humanities": 2,
      "mmlu_international_law": 1.0,
      "mmlu_jurisprudence": 1.0,
      "mmlu_logical_fallacies": 1.0,
      "mmlu_machine_learning": 1.0,
      "mmlu_management": 1.0,
      "mmlu_marketing": 1.0,
      "mmlu_medical_genetics": 1.0,
      "mmlu_miscellaneous": 1.0,
      "mmlu_moral_disputes": 1.0,
      "mmlu_moral_scenarios": 1.0,
      "mmlu_nutrition": 1.0,
      "mmlu_other": 2,
      "mmlu_philosophy": 1.0,
      "mmlu_prehistory": 1.0,
      "mmlu_professional_accounting": 1.0,
      "mmlu_professional_law": 1.0,
      "mmlu_professional_medicine": 1.0,
      "mmlu_professional_psychology": 1.0,
      "mmlu_public_relations": 1.0,
      "mmlu_security_studies": 1.0,
      "mmlu_social_sciences": 2,
      "mmlu_sociology": 1.0,
      "mmlu_stem": 2,
      "mmlu_us_foreign_policy": 1.0,
      "mmlu_virology": 1.0,
      "mmlu_world_religions": 1.0
    }
  },
  "metrics": {
    "accuracy": {
      "mathqa.acc,none": 0.1,
      "mathqa.acc_stderr,none": 0.09999999999999999,
      "mathqa.acc_norm,none": 0.2,
      "mathqa.acc_norm_stderr,none": 0.13333333333333333,
      "hellaswag.acc,none": 0.3,
      "hellaswag.acc_stderr,none": 0.15275252316519464,
      "hellaswag.acc_norm,none": 0.3,
      "hellaswag.acc_norm_stderr,none": 0.15275252316519464,
      "piqa.acc,none": 0.7,
      "piqa.acc_stderr,none": 0.15275252316519466,
      "piqa.acc_norm,none": 0.8,
      "piqa.acc_norm_stderr,none": 0.13333333333333333,
      "winogrande.acc,none": 0.8,
      "winogrande.acc_stderr,none": 0.13333333333333333,
      "mmlu.acc,none": 0.3456140350877193,
      "mmlu.acc_stderr,none": 0.019994527482197377,
      "mmlu_humanities.acc,none": 0.34615384615384615,
      "mmlu_humanities.acc_stderr,none": 0.04174056563102489,
      "mmlu_formal_logic.acc,none": 0.1,
      "mmlu_formal_logic.acc_stderr,none": 0.09999999999999999,
      "mmlu_high_school_european_history.acc,none": 0.5,
      "mmlu_high_school_european_history.acc_stderr,none": 0.16666666666666666,
      "mmlu_high_school_us_history.acc,none": 0.5,
      "mmlu_high_school_us_history.acc_stderr,none": 0.16666666666666666,
      "mmlu_high_school_world_history.acc,none": 0.5,
      "mmlu_high_school_world_history.acc_stderr,none": 0.16666666666666666,
      "mmlu_international_law.acc,none": 0.5,
      "mmlu_international_law.acc_stderr,none": 0.16666666666666666,
      "mmlu_jurisprudence.acc,none": 0.2,
      "mmlu_jurisprudence.acc_stderr,none": 0.13333333333333333,
      "mmlu_logical_fallacies.acc,none": 0.1,
      "mmlu_logical_fallacies.acc_stderr,none": 0.09999999999999999,
      "mmlu_moral_disputes.acc,none": 0.4,
      "mmlu_moral_disputes.acc_stderr,none": 0.16329931618554522,
      "mmlu_moral_scenarios.acc,none": 0.2,
      "mmlu_moral_scenarios.acc_stderr,none": 0.13333333333333333,
      "mmlu_philosophy.acc,none": 0.3,
      "mmlu_philosophy.acc_stderr,none": 0.15275252316519464,
      "mmlu_prehistory.acc,none": 0.5,
      "mmlu_prehistory.acc_stderr,none": 0.16666666666666666,
      "mmlu_professional_law.acc,none": 0.3,
      "mmlu_professional_law.acc_stderr,none": 0.15275252316519464,
      "mmlu_world_religions.acc,none": 0.4,
      "mmlu_world_religions.acc_stderr,none": 0.16329931618554522,
      "mmlu_other.acc,none": 0.3384615384615385,
      "mmlu_other.acc_stderr,none": 0.04134491152973615,
      "mmlu_business_ethics.acc,none": 0.2,
      "mmlu_business_ethics.acc_stderr,none": 0.13333333333333333,
      "mmlu_clinical_knowledge.acc,none": 0.4,
      "mmlu_clinical_knowledge.acc_stderr,none": 0.16329931618554522,
      "mmlu_college_medicine.acc,none": 0.3,
      "mmlu_college_medicine.acc_stderr,none": 0.15275252316519464,
      "mmlu_global_facts.acc,none": 0.6,
      "mmlu_global_facts.acc_stderr,none": 0.16329931618554522,
      "mmlu_human_aging.acc,none": 0.5,
      "mmlu_human_aging.acc_stderr,none": 0.16666666666666666,
      "mmlu_management.acc,none": 0.5,
      "mmlu_management.acc_stderr,none": 0.16666666666666666,
      "mmlu_marketing.acc,none": 0.3,
      "mmlu_marketing.acc_stderr,none": 0.15275252316519464,
      "mmlu_medical_genetics.acc,none": 0.3,
      "mmlu_medical_genetics.acc_stderr,none": 0.15275252316519464,
      "mmlu_miscellaneous.acc,none": 0.2,
      "mmlu_miscellaneous.acc_stderr,none": 0.13333333333333333,
      "mmlu_nutrition.acc,none": 0.4,
      "mmlu_nutrition.acc_stderr,none": 0.16329931618554522,
      "mmlu_professional_accounting.acc,none": 0.1,
      "mmlu_professional_accounting.acc_stderr,none": 0.09999999999999999,
      "mmlu_professional_medicine.acc,none": 0.5,
      "mmlu_professional_medicine.acc_stderr,none": 0.16666666666666666,
      "mmlu_virology.acc,none": 0.1,
      "mmlu_virology.acc_stderr,none": 0.09999999999999999,
      "mmlu_social_sciences.acc,none": 0.35,
      "mmlu_social_sciences.acc_stderr,none": 0.04356774205932811,
      "mmlu_econometrics.acc,none": 0.5,
      "mmlu_econometrics.acc_stderr,none": 0.16666666666666666,
      "mmlu_high_school_geography.acc,none": 0.2,
      "mmlu_high_school_geography.acc_stderr,none": 0.13333333333333333,
      "mmlu_high_school_government_and_politics.acc,none": 0.5,
      "mmlu_high_school_government_and_politics.acc_stderr,none": 0.16666666666666666,
      "mmlu_high_school_macroeconomics.acc,none": 0.4,
      "mmlu_high_school_macroeconomics.acc_stderr,none": 0.16329931618554522,
      "mmlu_high_school_microeconomics.acc,none": 0.0,
      "mmlu_high_school_microeconomics.acc_stderr,none": 0.0,
      "mmlu_high_school_psychology.acc,none": 0.3,
      "mmlu_high_school_psychology.acc_stderr,none": 0.15275252316519464,
      "mmlu_human_sexuality.acc,none": 0.6,
      "mmlu_human_sexuality.acc_stderr,none": 0.16329931618554522,
      "mmlu_professional_psychology.acc,none": 0.4,
      "mmlu_professional_psychology.acc_stderr,none": 0.16329931618554522,
      "mmlu_public_relations.acc,none": 0.3,
      "mmlu_public_relations.acc_stderr,none": 0.15275252316519464,
      "mmlu_security_studies.acc,none": 0.3,
      "mmlu_security_studies.acc_stderr,none": 0.15275252316519464,
      "mmlu_sociology.acc,none": 0.4,
      "mmlu_sociology.acc_stderr,none": 0.16329931618554522,
      "mmlu_us_foreign_policy.acc,none": 0.3,
      "mmlu_us_foreign_policy.acc_stderr,none": 0.15275252316519464,
      "mmlu_stem.acc,none": 0.3473684210526316,
      "mmlu_stem.acc_stderr,none": 0.03499989007589474,
      "mmlu_abstract_algebra.acc,none": 0.5,
      "mmlu_abstract_algebra.acc_stderr,none": 0.16666666666666666,
      "mmlu_anatomy.acc,none": 0.2,
      "mmlu_anatomy.acc_stderr,none": 0.13333333333333333,
      "mmlu_astronomy.acc,none": 0.3,
      "mmlu_astronomy.acc_stderr,none": 0.15275252316519464,
      "mmlu_college_biology.acc,none": 0.3,
      "mmlu_college_biology.acc_stderr,none": 0.15275252316519464,
      "mmlu_college_chemistry.acc,none": 0.4,
      "mmlu_college_chemistry.acc_stderr,none": 0.16329931618554522,
      "mmlu_college_computer_science.acc,none": 0.5,
      "mmlu_college_computer_science.acc_stderr,none": 0.16666666666666666,
      "mmlu_college_mathematics.acc,none": 0.2,
      "mmlu_college_mathematics.acc_stderr,none": 0.13333333333333333,
      "mmlu_college_physics.acc,none": 0.3,
      "mmlu_college_physics.acc_stderr,none": 0.15275252316519464,
      "mmlu_computer_security.acc,none": 0.3,
      "mmlu_computer_security.acc_stderr,none": 0.15275252316519464,
      "mmlu_conceptual_physics.acc,none": 0.2,
      "mmlu_conceptual_physics.acc_stderr,none": 0.13333333333333333,
      "mmlu_electrical_engineering.acc,none": 0.6,
      "mmlu_electrical_engineering.acc_stderr,none": 0.16329931618554522,
      "mmlu_elementary_mathematics.acc,none": 0.2,
      "mmlu_elementary_mathematics.acc_stderr,none": 0.13333333333333333,
      "mmlu_high_school_biology.acc,none": 0.6,
      "mmlu_high_school_biology.acc_stderr,none": 0.16329931618554522,
      "mmlu_high_school_chemistry.acc,none": 0.5,
      "mmlu_high_school_chemistry.acc_stderr,none": 0.16666666666666666,
      "mmlu_high_school_computer_science.acc,none": 0.3,
      "mmlu_high_school_computer_science.acc_stderr,none": 0.15275252316519464,
      "mmlu_high_school_mathematics.acc,none": 0.4,
      "mmlu_high_school_mathematics.acc_stderr,none": 0.16329931618554522,
      "mmlu_high_school_physics.acc,none": 0.2,
      "mmlu_high_school_physics.acc_stderr,none": 0.13333333333333333,
      "mmlu_high_school_statistics.acc,none": 0.3,
      "mmlu_high_school_statistics.acc_stderr,none": 0.15275252316519464,
      "mmlu_machine_learning.acc,none": 0.3,
      "mmlu_machine_learning.acc_stderr,none": 0.15275252316519464
    },
    "accuracy_avg": 0.24807561252235294
  },
  "ragas": {
    "skipped": true
  }
}