from flask import Flask, request, jsonify, Response, send_from_directory
from flask_cors import CORS
import time
import json
import os
import multiprocessing as mp

app = Flask(__name__)
CORS(app)

# Path to client dist (after running `npm run build` in webapp/client)
ROOT = os.path.dirname(__file__)
# project root (parent of webapp)
PROJECT_ROOT = os.path.normpath(os.path.join(ROOT, '..'))
# serve client from top-level project `client/dist`
CLIENT_DIST = os.path.join(PROJECT_ROOT, 'client', 'dist')

# Simple in-memory models list. In a real setup this should query available models / config.
AVAILABLE_MODELS = [
    "ibm-granite/granite-4.0-350M",
    "ibm-granite/granite-4.0-1b",
    "google/gemma-3-1b-it",
    "meta-llama/Llama-3.2-1B-Instruct",
    "gpt2",
    "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    "facebook/opt-125m"

]

# Default model name
DEFAULT_MODEL = "ibm-granite/granite-4.0-350M"

# In-memory cache for loaded tokenizers/models to avoid re-loading
MODEL_CACHE = {}


def get_model_and_tokenizer(model_name):
    """Load (or return cached) tokenizer and model for `model_name`.
    Returns (tokenizer, model, device). Raises RuntimeError if transformers/torch not available or load fails.
    """
    if model_name in MODEL_CACHE:
        return MODEL_CACHE[model_name]

    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        import torch
    except Exception as e:
        raise RuntimeError(f"transformers or torch not available: {e}")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Load tokenizer and model (cache them)
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
    model = AutoModelForCausalLM.from_pretrained(model_name)

    # Ensure pad_token_id is set to eos_token_id if missing (helps some models)
    if getattr(model.config, 'pad_token_id', None) is None and getattr(model.config, 'eos_token_id', None) is not None:
        model.config.pad_token_id = model.config.eos_token_id

    model.to(device)
    MODEL_CACHE[model_name] = (tokenizer, model, device)
    return MODEL_CACHE[model_name]


def fake_stream_response(prompt, model_name):
    """Yield tokens as Server-Sent Events (SSE). This is a fake streamer to simulate token-by-token arrivals."""
    # SSE format: data: <json>\n\n
    # Simulate some latency before first token
    time.sleep(0.5)
    tokens = (prompt + " \nGenerated by " + model_name).split()
    for i, tok in enumerate(tokens):
        now = time.time()
        payload = {
            "token": tok,
            "index": i,
            "is_last": (i == len(tokens) - 1),
            "timestamp": now,
        }
        yield f"data: {json.dumps(payload)}\n\n"
        # variable delay to simulate streaming speed
        time.sleep(0.05 + (i % 3) * 0.02)


def real_stream_response(prompt, model_name, max_new_tokens=128, temperature=0.7, top_p=0.9):
    """Stream generated tokens from a HF transformers model using TextIteratorStreamer.

    Yields SSE-style data lines containing JSON with keys: token, index, is_last, timestamp.
    Falls back to fake_stream_response on any failure.
    """
    try:
        from transformers import TextIteratorStreamer
        import threading
        import torch
    except Exception:
        # transformers not available -> fallback
        yield from fake_stream_response(prompt, model_name)
        return

    try:
        tokenizer, model, device = get_model_and_tokenizer(model_name)
    except Exception:
        # loading failed -> fallback
        yield from fake_stream_response(prompt, model_name)
        return

    # Prepare inputs
    try:
        inputs = tokenizer(prompt, return_tensors='pt')
        inputs = {k: v.to(device) for k, v in inputs.items()}
    except Exception:
        yield from fake_stream_response(prompt, model_name)
        return

    # Create a TextIteratorStreamer to stream decoded text
    try:
        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
    except Exception:
        yield from fake_stream_response(prompt, model_name)
        return

    gen_kwargs = dict(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=temperature,
        top_p=top_p,
        streamer=streamer,
        return_dict_in_generate=True,
    )

    # Run generation in a background thread so we can iterate over streamer
    def _generate():
        try:
            model.generate(**gen_kwargs)
        except Exception:
            # generation error; streamer iteration will end
            pass

    thread = threading.Thread(target=_generate, daemon=True)
    thread.start()

    index = 0
    # streamer yields decoded text chunks. We'll split into whitespace tokens for per-token events.
    try:
        for chunk in streamer:
            if not chunk:
                continue
            parts = chunk.split()
            for tok in parts:
                now = time.time()
                payload = {
                    "token": tok,
                    "index": index,
                    "is_last": False,
                    "timestamp": now,
                }
                yield f"data: {json.dumps(payload)}\n\n"
                index += 1
    except Exception:
        # if streamer fails, gracefully fall back
        yield from fake_stream_response(prompt, model_name)
        return

    # wait for generation thread to finish
    thread.join(timeout=1.0)
    # send final is_last=True event
    now = time.time()
    yield f"data: {json.dumps({'token': '', 'index': index, 'is_last': True, 'timestamp': now})}\n\n"


# Worker management
WORKERS = {}
WORKER_STATUS = {}
CANCEL_DICT = None

def _worker_entry(req_q, res_q, model_name, cpu_aff, cancel_dict):
    """Entry point used as the multiprocessing target.
    This function is defined at module top-level so it can be pickled by the 'spawn'
    start method. It imports worker.py as a plain module after ensuring the
    webapp directory is on sys.path, then delegates to worker_main.
    """
    # Ensure the webapp directory is on sys.path so `import worker` resolves
    try:
        import sys
        if ROOT not in sys.path:
            sys.path.insert(0, ROOT)
    except Exception:
        pass

    # Import worker.py as a plain module. This requires `worker.py` to be importable
    # from the webapp directory (we ensured ROOT is on sys.path above).
    try:
        import worker as worker_mod
    except Exception as e:
        # Raise a clear error so the caller can see why the worker failed to start
        raise RuntimeError(f"Failed to import worker module: {e}")

    # Delegate to worker_main, passing cancel_dict
    worker_mod.worker_main(req_q, res_q, model_name, cpu_aff, cancel_dict)


def start_workers(models_list, cancel_dict=None):
    """Start worker processes for each model in models_list.
    Each worker gets a pair of queues (request_q, response_q) and is started via _worker_entry.
    """
    # Read affinities from env
    affinity_map = {}
    try:
        raw = os.environ.get('WORKER_CPU_AFFINITY')
        if raw:
            import json as _json
            affinity_map = _json.loads(raw)
    except Exception:
        affinity_map = {}

    for m in models_list:
        if m in WORKERS:
            continue
        req_q = mp.Queue()
        res_q = mp.Queue()
        cpu_aff = affinity_map.get(m)
        # use top-level _worker_entry as target so spawn can pickle it
        p = mp.Process(target=_worker_entry, args=(req_q, res_q, m, cpu_aff, cancel_dict), daemon=True)
        p.start()
        WORKERS[m] = {"proc": p, "req_q": req_q, "res_q": res_q}
        WORKER_STATUS[m] = {"state": "starting", "pid": p.pid, "model": m}
        # Try to read any immediate bootstrap messages (model_ready or worker_error)
        try:
            while True:
                msg = res_q.get(timeout=0.2)
                if isinstance(msg, dict) and msg.get('id') is None:
                    et = msg.get('event_type')
                    if et == 'model_ready':
                        WORKER_STATUS[m]['state'] = 'ready'
                        WORKER_STATUS[m]['ready_ts'] = msg.get('payload', {}).get('timestamp')
                    elif et == 'worker_error':
                        WORKER_STATUS[m]['state'] = 'error'
                        WORKER_STATUS[m]['error'] = msg.get('payload')
        except Exception:
            pass


def stop_workers():
    for m, info in WORKERS.items():
        try:
            info['req_q'].put(None)
            info['proc'].join(timeout=2.0)
        except Exception:
            pass
    WORKERS.clear()
    WORKER_STATUS.clear()


# NOTE: don't start workers at import time on Windows (spawn start method).
# Worker processes are started from the protected main block below to avoid
# the "start new process before current process finished bootstrapping" error.
# start_workers(AVAILABLE_MODELS)  # moved to main guard


@app.route('/models', methods=['GET'])
def list_models():
    return jsonify({"models": AVAILABLE_MODELS})


@app.route('/chat', methods=['POST'])
def chat():
    data = request.json or {}
    prompt = data.get('prompt', '')
    model_name = data.get('model', DEFAULT_MODEL)

    # Read generation params from request body with safe defaults and validation
    try:
        max_new_tokens = int(data.get('max_new_tokens', 128))
    except Exception:
        max_new_tokens = 128
    max_new_tokens = max(1, min(max_new_tokens, 2048))

    try:
        temperature = float(data.get('temperature', 0.7))
    except Exception:
        temperature = 0.7
    temperature = max(0.0, min(temperature, 5.0))

    try:
        top_p = float(data.get('top_p', 0.9))
    except Exception:
        top_p = 0.9
    top_p = max(0.0, min(top_p, 1.0))

    # do_sample flag
    do_sample = data.get('do_sample', True)
    try:
        do_sample = bool(do_sample)
    except Exception:
        do_sample = True

    # If a worker exists for the model, route the request there
    worker_info = WORKERS.get(model_name)
    if worker_info:
        req_id = str(time.time()) + '_' + str(os.getpid())
        payload = {"id": req_id, "prompt": prompt, "max_new_tokens": max_new_tokens, "temperature": temperature, "top_p": top_p, "do_sample": do_sample}
        worker_info['req_q'].put(payload)

        def gen():
            # Immediately inform client of assigned request id
            yield f"data: {json.dumps({'event':'assigned','payload':{'req_id': req_id}})}\n\n"
            # stream responses from worker's response queue that match req_id
            res_q = worker_info['res_q']
            while True:
                try:
                    msg = res_q.get()
                except Exception:
                    break
                if not isinstance(msg, dict):
                    continue
                # worker-level messages have id == None
                if msg.get('id') is None:
                    # update worker status if needed
                    et = msg.get('event_type')
                    if et == 'model_ready':
                        WORKER_STATUS[model_name]['state'] = 'ready'
                        WORKER_STATUS[model_name]['ready_ts'] = msg.get('payload', {}).get('timestamp')
                    elif et == 'worker_error':
                        WORKER_STATUS[model_name]['state'] = 'error'
                        WORKER_STATUS[model_name]['error'] = msg.get('payload')
                    yield f"data: {json.dumps({ 'event': msg.get('event_type'), 'payload': msg.get('payload') })}\n\n"
                    if msg.get('event_type') == 'worker_error':
                        break
                    continue

                # if message is for another request, re-queue it
                if msg.get('id') not in (req_id, None):
                    res_q.put(msg)
                    time.sleep(0.01)
                    continue

                event_type = msg.get('event_type')
                payload = msg.get('payload')
                yield f"data: {json.dumps({ 'event': event_type, 'payload': payload })}\n\n"
                if event_type == 'done' or event_type == 'cancelled':
                    break
            # end of generator
        return Response(gen(), mimetype='text/event-stream')

    # Fallback to in-process generator (real_stream_response or fake)
    try:
        return Response(real_stream_response(prompt, model_name, max_new_tokens=max_new_tokens, temperature=temperature, top_p=top_p), mimetype='text/event-stream')
    except Exception:
        return Response(fake_stream_response(prompt, model_name), mimetype='text/event-stream')


@app.route('/cancel', methods=['POST'])
def cancel_request():
    data = request.json or {}
    req_id = data.get('req_id')
    if not req_id:
        resp = jsonify({'ok': False, 'error': 'missing req_id'})
        resp.status_code = 400
        return resp
    if CANCEL_DICT is None:
        resp = jsonify({'ok': False, 'error': 'cancel not available'})
        resp.status_code = 500
        return resp
    # set cancellation flag
    try:
        CANCEL_DICT.update({req_id: True})
        return jsonify({'ok': True})
    except Exception as e:
        resp = jsonify({'ok': False, 'error': str(e)})
        resp.status_code = 500
        return resp


@app.route('/workers', methods=['GET'])
def workers_status():
    return jsonify(WORKER_STATUS)


@app.route('/compare', methods=['POST'])
def compare_texts():
    """Compare two assistant outputs (left and right) using simple whitespace tokenization.
    Returns token counts, shared token counts, Jaccard index, and length difference.
    This endpoint is intentionally minimal and safe for NWO usage.
    """
    data = request.json or {}
    left = (data.get('left') or '')
    right = (data.get('right') or '')

    # Basic normalization: strip and lower to make overlap detection case-insensitive
    left_norm = left.strip()
    right_norm = right.strip()

    # Tokenize by whitespace. This is intentionally simple; front-end expects these fields.
    left_tokens = [t for t in left_norm.split() if t]
    right_tokens = [t for t in right_norm.split() if t]

    left_set = set(left_tokens)
    right_set = set(right_tokens)

    shared = left_set.intersection(right_set)
    union = left_set.union(right_set)

    left_count = len(left_tokens)
    right_count = len(right_tokens)
    shared_count = len(shared)
    union_count = len(union) if union else 0

    jaccard = (shared_count / union_count) if union_count else 0.0
    length_diff = left_count - right_count

    result = {
        'left_tokens': left_count,
        'right_tokens': right_count,
        'shared_tokens': shared_count,
        'jaccard': jaccard,
        'length_diff': length_diff,
        'note': 'Simple whitespace token overlap (case-sensitive)'
    }

    return jsonify(result)

# Serve the client (if built) --------------------------------------------------
@app.route('/', defaults={'path': ''})
@app.route('/<path:path>')
def serve_client(path):
    """Serve the built React app from client/dist. If the dist folder doesn't exist, return a helpful message."""
    if os.path.isdir(CLIENT_DIST):
        # If requesting root or a path that doesn't exist in dist, serve index.html (for SPA routing)
        requested = path or 'index.html'
        requested_path = os.path.join(CLIENT_DIST, requested)
        if os.path.exists(requested_path) and os.path.isfile(requested_path):
            return send_from_directory(CLIENT_DIST, requested)
        # default to index.html
        return send_from_directory(CLIENT_DIST, 'index.html')

    # If client hasn't been built, provide instructions
    message = {
        "error": "Client not built",
        "instructions": [
            "cd webapp/client",
            "npm install",
            "npm run build",
            "Then start this Flask server to serve the built client: python webapp/server.py",
            "Alternatively run the client in dev mode with Vite: cd webapp/client && npm run dev (and keep this server running separately)"
        ]
    }
    return jsonify(message), 400


if __name__ == '__main__':
    # Support multiprocessing on Windows frozen executables
    mp.freeze_support()

    # On Windows the 'spawn' start method is used; ensure it's set before
    # creating processes and avoid starting workers at import-time.
    try:
        mp.set_start_method('spawn')
    except RuntimeError:
        # start method already set
        pass

    # Create a Manager and a shared cancel dict for request cancellation
    manager = mp.Manager()
    CANCEL_DICT = manager.dict()

    # Start workers now (in the guarded main context), passing cancel dict
    start_workers(AVAILABLE_MODELS, cancel_dict=CANCEL_DICT)

    try:
        # Disable the Flask reloader so we don't accidentally start workers twice.
        # For development you can set debug=False or run the client with Vite.
        app.run(host='0.0.0.0', port=7860, debug=True, use_reloader=False)
    finally:
        # Cleanly stop worker processes on exit
        stop_workers()
