_target_: refrag.framework.apps.train.TrainerApp

model_name: meta-llama/Llama-3.1-8B-Instruct
#model_name: meta-llama/Llama-3.2-1B-Instruct
#model_name: ibm-granite/granite-4.0-350M
train:
  epochs: 1
  steps_per_epoch: 10
  batch_size: 1
  lr: 1e-4
  grad_clip_norm: 1.0
  sched_step_size: 100
  sched_gamma: 0.9
  log_dir: ${hydra:run.dir}/runs/alignment


datamix:
  datasets:
    - name: HuggingFaceH4/ultrachat_200k
      split: train_sft
      weight: 1.0
  max_samles_per_dataset: 5000


training_args:
  output_dir: ${hydra:run.dir}/sft
  per_device_train_batch_size: 2
  num_train_epochs: 3
  max_steps: 9
  logging_steps: 20
  report_to: tensorboard
  bf16: true
  #evaluation_strategy: steps
  #eval_steps: 200

max_seq_length: 128
packing: false
